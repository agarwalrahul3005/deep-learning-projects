{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e826906",
   "metadata": {
    "id": "0e826906"
   },
   "source": [
    "### The Smart Supplier: Optimizing Orders in a Fluctuating Market - 6 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beffcce",
   "metadata": {
    "id": "2beffcce"
   },
   "source": [
    "### Group ID:\n",
    "Group ID: Group 278\n",
    "### Group Members Name with Student ID:\n",
    "1. Rahul Prashar - 2024AB05058 - 100%\n",
    "2. Rahul Agarwal - 2024AA05676 - 100%\n",
    "3. Rahul Sinha - 2024AA05036 - 100%\n",
    "4. Raghavendra Sathakarni A S - 2024AB05118 - 100%\n",
    "\n",
    "### Assginment 1 - Part 2\n",
    "\n",
    "Develop a reinforcement learning agent using dynamic programming to help a Smart Supplier decide which products to manufacture and sell each day to maximize profit. The agent must learn the optimal policy for choosing daily production quantities, considering its limited raw materials and the unpredictable daily demand and selling prices for different products.\n",
    "\n",
    "#### **Scenario**\n",
    " A small Smart Supplier manufactures two simple products: Product A and Product B. Each day, the supplier has a limited amount of raw material. The challenge is that the market demand and selling price for Product A and Product B change randomly each day, making some products more profitable than others at different times. The supplier needs to decide how much of each product to produce to maximize profit while managing their limited raw material.\n",
    "\n",
    "#### **Objective**\n",
    "The Smart Supplier's agent must learn the optimal policy π∗ using dynamic programming (Value Iteration or Policy Iteration) to decide how many units of Product A and Product B to produce each day to maximize the total profit over the fixed number of days, given the daily changing market conditions and limited raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4feac",
   "metadata": {
    "id": "15f4feac"
   },
   "source": [
    "### 1. Custom Environment Creation (SmartSupplierEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50XfS5Hp380",
   "metadata": {
    "id": "f50XfS5Hp380"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a4a087",
   "metadata": {
    "id": "89a4a087"
   },
   "outputs": [],
   "source": [
    "# Define market states and their product prices\n",
    "# Structure: {Market_State_ID: {'A_price': X, 'B_price': Y}}\n",
    "# Define product raw material costs\n",
    "# Define actions: (num_A, num_B, raw_material_cost_precalculated)\n",
    "        # Action ID mapping:\n",
    "        # 0: Produce_2A_0B\n",
    "        # 1: Produce_1A_2B\n",
    "        # 2: Produce_0A_5B\n",
    "        # 3: Produce_3A_0B\n",
    "        # 4: Do_Nothing\n",
    "\n",
    " # Define state space dimensions\n",
    "        # Current Day: 1 to num_days\n",
    "        # Current Raw Material: 0 to initial_raw_material\n",
    "        # Current Market State: 1 or 2\n",
    "\n",
    "# get reward function\n",
    "class SmartSupplierEnv:\n",
    "    \"\"\"\n",
    "    Environment modeling a Smart Supplier with fluctuating daily market prices\n",
    "    and limited raw material to produce two products A and B.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_days=5, init_rm=10):\n",
    "        \"\"\"\n",
    "        Initialize the environment parameters.\n",
    "        Args:\n",
    "            num_days (int): Total number of days in an episode.\n",
    "            init_rm (int): Initial units of raw material available each day.\n",
    "        \"\"\"\n",
    "        self.num_days = num_days\n",
    "        self.init_rm = init_rm\n",
    "        # Define market states and their selling prices\n",
    "        # Market state 1: High demand for A; state 2: High demand for B\n",
    "        self.market_states = [1, 2]\n",
    "        self.prices = {\n",
    "            1: {'A': 8, 'B': 2},   # Market State 1 prices\n",
    "            2: {'A': 3, 'B': 5},   # Market State 2 prices\n",
    "        }\n",
    "        # Define actions mapping: action_id -> (units_A, units_B, rm_cost)\n",
    "        self.actions = {\n",
    "            0: (2, 0, 2*2),         # Produce 2A, 0B costing 4 RM\n",
    "            1: (1, 2, 1*2 + 2*1),   # Produce 1A, 2B costing 4 RM\n",
    "            2: (0, 5, 5*1),         # Produce 0A, 5B costing 5 RM\n",
    "            3: (3, 0, 3*2),         # Produce 3A, 0B costing 6 RM\n",
    "            4: (0, 0, 0),           # Do nothing\n",
    "        }\n",
    "\n",
    "    def get_reward(self, market, action):\n",
    "        \"\"\"\n",
    "        Compute the profit reward for taking a given action in a market state.\n",
    "        Args:\n",
    "            market (int): Current market state (1 or 2).\n",
    "            action (int): Action ID.\n",
    "        Returns:\n",
    "            int: Profit earned (zero if invalid or do-nothing).\n",
    "        \"\"\"\n",
    "        units_A, units_B, cost = self.actions[action]\n",
    "        # Price per unit from current market state\n",
    "        price_A = self.prices[market]['A']\n",
    "        price_B = self.prices[market]['B']\n",
    "        return units_A * price_A + units_B * price_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b2487",
   "metadata": {
    "id": "c89b2487"
   },
   "source": [
    "### 2. Dynamic Programming Implementation (Value Iteration or Policy Iteration) (2 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027db857",
   "metadata": {
    "id": "027db857"
   },
   "outputs": [],
   "source": [
    "# Value Iteration function\n",
    "def value_iteration(env, gamma=1.0, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Perform Value Iteration over the state space to compute optimal V* and policy.\n",
    "    Args:\n",
    "        env (SmartSupplierEnv): The Smart Supplier environment.\n",
    "        gamma (float): Discount factor (1.0 for episodic sum maximization).\n",
    "        theta (float): Convergence threshold.\n",
    "    Returns:\n",
    "        V (dict): Mapping from (day, rm, market) -> value.\n",
    "        policy (dict): Mapping from (day, rm, market) -> best action ID.\n",
    "    \"\"\"\n",
    "    V = {}       # State-value function\n",
    "    policy = {}  # Optimal policy mapping\n",
    "    # Initialize V(s) = 0 for all states\n",
    "    for day in range(1, env.num_days + 1):\n",
    "        for rm in range(env.init_rm + 1):\n",
    "            for m in env.market_states:\n",
    "                V[(day, rm, m)] = 0.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        # Sweep over all states\n",
    "        for day in range(1, env.num_days + 1):\n",
    "            for rm in range(env.init_rm + 1):\n",
    "                for m in env.market_states:\n",
    "                    state = (day, rm, m)\n",
    "                    v_old = V[state]\n",
    "                    action_values = {}\n",
    "                    # Evaluate each action\n",
    "                    for a, (_, _, cost) in env.actions.items():\n",
    "                        # If not enough raw material, reward is zero\n",
    "                        if cost > rm:\n",
    "                            reward = 0\n",
    "                        else:\n",
    "                            reward = env.get_reward(m, a)\n",
    "                        # Next state transition\n",
    "                        next_day = day + 1\n",
    "                        if next_day > env.num_days:\n",
    "                            future = 0.0\n",
    "                        else:\n",
    "                            # Expectation over next day's market shift (50/50)\n",
    "                            future = 0.0\n",
    "                            for m2 in env.market_states:\n",
    "                                future += 0.5 * V[(next_day, env.init_rm, m2)]\n",
    "                        action_values[a] = reward + gamma * future\n",
    "                    # Pick best action\n",
    "                    best_action = max(action_values, key=action_values.get)\n",
    "                    V[state] = action_values[best_action]\n",
    "                    policy[state] = best_action\n",
    "                    delta = max(delta, abs(v_old - V[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cfa71",
   "metadata": {
    "id": "b39cfa71"
   },
   "source": [
    "### 3. Simulation and Policy Analysis ( 1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62490896",
   "metadata": {
    "id": "62490896"
   },
   "outputs": [],
   "source": [
    "# simulate policy function - Simulates the learned policy over multiple runs to evaluate performance\n",
    "\n",
    "# analyze policy function - Analyzes and prints snippets of the learned optimal policy\n",
    "def simulate_policy(env, policy, episodes=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Simulate the learned policy to estimate average total profit.\n",
    "    Args:\n",
    "        env (SmartSupplierEnv): Environment instance.\n",
    "        policy (dict): Optimal policy mapping states to actions.\n",
    "        episodes (int): Number of simulation runs.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        float: Average total profit over all episodes.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Ensure reproducibility\n",
    "    total_profits = []\n",
    "    for _ in range(episodes):\n",
    "        profit = 0\n",
    "        # Random initial market state each episode\n",
    "        m = random.choice(env.market_states)\n",
    "        # Run through days\n",
    "        for day in range(1, env.num_days + 1):\n",
    "            state = (day, env.init_rm, m)\n",
    "            a = policy[state]\n",
    "            # Apply action if RM constraint allows\n",
    "            if env.actions[a][2] <= env.init_rm:\n",
    "                profit += env.get_reward(m, a)\n",
    "            # Market shifts for next day\n",
    "            m = random.choice(env.market_states)\n",
    "        total_profits.append(profit)\n",
    "    # Return mean profit\n",
    "    return sum(total_profits) / episodes\n",
    "\n",
    "\n",
    "def analyze_policy(policy, env):\n",
    "    \"\"\"\n",
    "    Print key insights of the optimal policy for selected states.\n",
    "    \"\"\"\n",
    "    print(\"\\nPolicy snippets at Day 1:\")\n",
    "    for market in env.market_states:\n",
    "        for rm in [0, env.init_rm//2, env.init_rm]:\n",
    "            action = policy[(1, rm, market)]\n",
    "            units = env.actions[action][:2]\n",
    "            print(f\"Market={market}, RM={rm} -> Action {action}, produce A={units[0]}, B={units[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305647e",
   "metadata": {
    "id": "3305647e"
   },
   "source": [
    "### Performance Evaluation: (1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e6570fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6570fe",
    "outputId": "f454ecc9-6e9f-49bf-d8f3-fc90186ffd97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V*(Day1, RM=10, Market=1) = 122.00\n",
      "\n",
      "Policy snippets at Day 1:\n",
      "Market=1, RM=0 -> Action 0, produce A=2, B=0\n",
      "Market=1, RM=5 -> Action 0, produce A=2, B=0\n",
      "Market=1, RM=10 -> Action 3, produce A=3, B=0\n",
      "Market=2, RM=0 -> Action 0, produce A=2, B=0\n",
      "Market=2, RM=5 -> Action 2, produce A=0, B=5\n",
      "Market=2, RM=10 -> Action 2, produce A=0, B=5\n",
      "Average profit over 1000 runs: 122.50\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    env2 = SmartSupplierEnv(num_days=5, init_rm=10)\n",
    "    \n",
    "    # Compute optimal values and policy via Value Iteration\n",
    "    V_opt, policy_opt = value_iteration(env2)\n",
    "    \n",
    "    # Display key value\n",
    "    print(f\"V*(Day1, RM=10, Market=1) = {V_opt[(1,10,1)]:.2f}\")\n",
    "    \n",
    "    # Show policy examples\n",
    "    analyze_policy(policy_opt, env2)\n",
    "    \n",
    "    # Simulate policy performance\n",
    "    avg_profit = simulate_policy(env2, policy_opt, episodes=1000)\n",
    "    print(f\"Average profit over 1000 runs: {avg_profit:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2be90-5a40-4eb0-b157-c44ab86eb0f0",
   "metadata": {},
   "source": [
    "### 5. Impact of Dynamics Analysis (1 Mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e07f1-e5c7-4626-b5f0-89af4d87c40b",
   "metadata": {},
   "source": [
    "#### Discusses the impact of dynamic market prices on the optimal policy.\n",
    "##### The learned optimal policy adapts strategically to daily market fluctuations:\n",
    "* 1. **Market Sensitivity** – When the market state favors Product A (State 1), the policy\n",
    "    allocates more raw material to produce A, maximizing high-margin returns, and\n",
    "    vice versa for Product B in State 2. This dynamic switching captures the core\n",
    "    benefit of reacting to price signals.\n",
    "* 2. **Resource Conservation** – On days with limited remaining raw material or earlier\n",
    "   in the horizon, the policy may choose smaller bundles or even Do_Nothing to\n",
    "    preserve capacity, anticipating a potential shift to a more lucrative market.\n",
    "* 3. **Horizon Effects** – As the final day approaches, the policy becomes more aggressive,\n",
    "    exhausting all available raw material regardless of state, since there is no future\n",
    "   value to conserve. This results in maximal immediate profit extraction.\n",
    "* 4. **Robustness to Uncertainty** – By embedding a 50/50 expectation over next-day market\n",
    "    states in the value iteration, the policy inherently balances expected gains across\n",
    "    both possibilities, avoiding overcommitment to one product when future states are\n",
    "    ambiguous.\n",
    "\n",
    "Overall, dynamic programming produces a policy that intelligently balances exploitation of current high-price conditions with cautious preservation of resources for uncertain future opportunities, thereby maximizing cumulative profit over time."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
